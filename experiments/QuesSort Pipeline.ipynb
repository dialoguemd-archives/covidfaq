{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions Clustering - English\n",
    "\n",
    "## Expected\n",
    "Questions to be sorted out such that the response to the whole cluster is same.\n",
    "\n",
    "<!--### To Do-->\n",
    "Author: Sunanda Bansal  \n",
    "Organization: Dataperformers  \n",
    "License: CC BY-NC  \n",
    "Date: 24 Mar, 2020 (Start)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import regex\n",
    "import os\n",
    "import csv\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import scipy\n",
    "import socket\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import multiprocessing as mp\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import Normalizer   \n",
    "from sklearn import metrics   \n",
    "from sklearn.metrics import confusion_matrix\n",
    "# from sklearn.metrics.pairwise import paired_distances as sklearn_paired_distances\n",
    "\n",
    "# Plotting\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# importing personal development helper classes\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define variables here\n",
    "\n",
    "Mostly the code will be intended to use with arguments that can be passed in comman line, but jupyter notebook doesn't handle `argparse` well, so the Args class is a temporary way to write the code assumming the variables to be an attribute of an object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    def __init__(self):\n",
    "        # The very big scraped file, give absolute path, outside the repo\n",
    "        # self.filename = \"query_result_2020-03-27T19_12_30.866993Z.csv\"\n",
    "        self.filename = \"covid_questions_2020-04-21.csv\"\n",
    "        \n",
    "        # path to the file\n",
    "        self.dataset = f\"data/{self.filename}\"    \n",
    "        \n",
    "        # suffix used to create\n",
    "        self.suffix = \"_\".join([word for word in self.filename.split(\"_\") if not word.isalpha()])[:-4]\n",
    "        self.vector_mode = \"tfidf\"\n",
    "        self.n_topics = 230\n",
    "        self.dist_thresh = 1.5\n",
    "        self.lang = \"en\"\n",
    "args=Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This dictionary is used to translate old labels to new labels minimizing modifications required for the moment\n",
    "new_labels = {\n",
    "                \"about\": \"covid-what\",\n",
    "                \"animals\": \"covid-animals\",\n",
    "                \"caution\": \"personal-caution\",\n",
    "                \"dangerisk\": \"covid-contagious\",\n",
    "                \"diff\": \"covid-versus\",\n",
    "                \"future\": \"situation-future\",\n",
    "                \"guideme\": \"personal-whatif\",\n",
    "                \"incubation\": \"covid-incubation\",\n",
    "                \"infection\": \"covid-infection\",\n",
    "                \"isolation\": \"personal-isolation\",\n",
    "                \"lockdown\": \"situation-lockdown\",\n",
    "                \"nextsteps\": \"personal-symptoms\",\n",
    "                \"past\": \"situation-past\",\n",
    "                \"recover\": \"covid-recovery\",\n",
    "                \"statistics\": \"situation-stats\",\n",
    "                \"symptom\": \"covid-symptoms\",\n",
    "                \"test\": \"personal-testing\",\n",
    "                \"transmission\": \"covid-transmission\",\n",
    "                \"treatment\": \"covid-med\",\n",
    "                \"unclassified\": \"unclassified\",\n",
    "                \"virusfight\": \"covid-fight\",\n",
    "                \"viruskill\": \"covid-kill\",\n",
    "                \"viruslife\": \"covid-life\"\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable (FALSE) displaying warnings from the OpenMP* run-time library during program execution.\n",
    "os.environ['KMP_WARNINGS'] = \"FALSE\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regex functions\n",
    "def surrounding(word,area=2):\n",
    "    # A funtion, yet to be defined, that can be used to extract text around keywords\n",
    "    return\n",
    "\n",
    "def fuzzy_match(word,pattern):\n",
    "    '''\n",
    "        Fuzzy matching function to be used with .apply() of pandas\n",
    "\n",
    "        Reason - Fuzzy matching is available in regex package, not in re package, \n",
    "        therefore fuzzy matching is not a part of pandas string matching functions\n",
    "    '''\n",
    " \n",
    "    if regex.search(pattern, word, re.IGNORECASE):\n",
    "        return True\n",
    "    else:\n",
    "        return False    \n",
    "    \n",
    "# Language Detection\n",
    "from langdetect import detect\n",
    "def detect_lang(text):\n",
    "    # Used to detect language of the question\n",
    "    try:\n",
    "        return detect(text)\n",
    "    except:\n",
    "        return \"unidentifiable\"   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Natural Lanuage Proprocessing Functions\n",
    "   \n",
    "Preprocessing done -\n",
    "   1. Normalizing accents  \n",
    "   2. Removing non alphabetic characters  \n",
    "   3. Casefolding  \n",
    "\n",
    "Preprocessing not done -\n",
    "   1. Stopword removal - For questions, stopwords are essential and thus are retained\n",
    "   2. Stemming - For rule based analysis it might be useful to keep the words as they are, for LSA, the questions don't have enough variation in content to benefit for stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import unidecode\n",
    "\n",
    "# Regular expression to select all that is not alphabet\n",
    "# @maybe allow numbers as well\n",
    "alpha_regex = re.compile('[^a-zA-Z]')\n",
    "\n",
    "from nltk.corpus import stopwords as sw\n",
    "if args.lang == \"en\": stopwords = sw.words('english')\n",
    "if args.lang == \"fr\": stopwords = sw.words('french')\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "if args.lang == \"en\": stemmer = SnowballStemmer(\"english\")\n",
    "if args.lang == \"fr\": stemmer = SnowballStemmer(\"french\")\n",
    "\n",
    "def tokenize(text):\n",
    "    '''\n",
    "        1. Normalized accents\n",
    "        2. Splits at non alpbhaetic character (@maybe need to revisit for french text)\n",
    "        3. Caasefolds\n",
    "    '''    \n",
    "    tokens = []\n",
    "    for sent in nltk.sent_tokenize(text):\n",
    "        for word in nltk.word_tokenize(sent):\n",
    "            # Handle french accents in text\n",
    "            word = unidecode.unidecode(word)\n",
    "            \n",
    "            # Split at every non alphabet character occurrence\n",
    "            clean_words = alpha_regex.sub(' ', word).split()\n",
    "            \n",
    "            # Casefold\n",
    "            tokens.extend([word.lower() for word in clean_words])\n",
    "    \n",
    "    # Return tokens\n",
    "    return tokens\n",
    "\n",
    "def stem(word):\n",
    "    return stemmer.stem(word).strip()\n",
    "\n",
    "def preprocess(text):    \n",
    "    tokenized = tokenize(text)\n",
    "    # cleaned = [word for word in tokenized if word not in stopwords and word != '']\n",
    "    # stemmed = [stem(word) for word in cleaned]\n",
    "    return ' '.join(tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset has 5054 documents\n",
      "Dataset has 2030 english documents and 2719 french documents\n"
     ]
    }
   ],
   "source": [
    "# Read dataset\n",
    "dataset = pd.read_csv(args.dataset)\n",
    "print(f\"Dataset has {len(dataset)} documents\")\n",
    "\n",
    "# Detect Language\n",
    "dataset[\"detected_lang\"] = dataset.question.apply(detect_lang)\n",
    "\n",
    "print(f\"Dataset has {len(dataset[dataset.detected_lang=='en'])} english documents and {len(dataset[dataset.detected_lang=='fr'])} french documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translate the output french to english using gogle translate and copy this into this file - data/covid_questions_2020-04-21_fr_translations.txt \n",
      "----------------------------------------------------------------------------------------------------\n",
      "Test \n",
      "Are you sick?\n",
      "Pouvez vous me répondre en français, sinon, je suis parfaitement bilingue, je travaille dans l’industrie de solutions technologie and it would be my pleasure to help this project any way I can. Jennifer Charron at Workday Canada from Montreal. \n",
      "Combien de temps va durer la quarantaine \n",
      "Est-ce vous avez mal a la gorge?\n",
      "À quand la Heineken virus? \n",
      "Bonjour!\n",
      "Bonjour\n",
      "Bonjour!\n",
      "Combien de temps dure la phase active de la maladie\n",
      "Exemple ?\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Press Enter to continue...\n"
     ]
    }
   ],
   "source": [
    "# Translation Questions from French to English\n",
    "if \"translated_question\" not in dataset.columns:\n",
    "    dataset[\"translated_question\"] = np.nan\n",
    "\n",
    "translated_df_path = f\"data/{os.path.basename(args.filename)[:-4]}_fr.csv\"\n",
    "fr = pd.DataFrame(columns=[\"question\",\"translated_question\"])\n",
    "if os.path.exists(translated_df_path):\n",
    "    fr_csv = pd.read_csv(translated_df_path, index_col=0)\n",
    "    fr = fr.combine_first(fr_csv)\n",
    "    dataset.update(fr.translated_question)\n",
    "    \n",
    "# Any french questions that need to be translated?\n",
    "fr_questions = dataset[\n",
    "                        (dataset.detected_lang == \"fr\") & \n",
    "                        (dataset.translated_question.isnull())\n",
    "                      ][[\"question\",\"translated_question\"]]\n",
    "\n",
    "if len(fr_questions)>0:\n",
    "    googletrans_fail = False\n",
    "    \n",
    "    try:\n",
    "        from googletrans import Translator\n",
    "        translator = Translator()\n",
    "        translations = []\n",
    "        for q in list(fr_questions.question):\n",
    "            translations.append(translator.translate(q))\n",
    "    except:\n",
    "        # HACK for when your IP has been blocked\n",
    "        googletrans_fail = True\n",
    "        try_again = \"yes\"\n",
    "        translations_file_path = f\"data/{os.path.basename(args.filename)[:-4]}_fr_translations.txt\"\n",
    "        if not os.path.exists(translations_file_path):    \n",
    "            os.mknod(translations_file_path)           \n",
    "\n",
    "        with open(translations_file_path,\"r\") as f:\n",
    "            translations = f.read().split(\"\\n\")  \n",
    "        \n",
    "        while (len(translations) != len(fr_questions)):\n",
    "            print(f\"Translate the output french to english using gogle translate and copy this into this file - {translations_file_path} \\n{'-'*100}\")\n",
    "            print(*fr_questions.question.tolist(),sep=\"\\n\")\n",
    "            # Translate the output french to english using gogle translate \n",
    "            # and copy this into a text file save as {filename}_fr_translated.txt in data folder\n",
    "            input(f\"{'-'*100}\\nPress Enter to continue...\")\n",
    "            \n",
    "            with open(translations_file_path,\"r\") as f:\n",
    "                translations = f.read().split(\"\\n\")  \n",
    "            \n",
    "    fr_questions.translated_question = translations\n",
    "    dataset.update(fr_questions.translated_question)\n",
    "    dataset[dataset.detected_lang == \"fr\"][[\"question\",\"translated_question\"]].to_csv(translated_df_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset has 4764 english documents (translated and otherwise)\n"
     ]
    }
   ],
   "source": [
    "# Filter questions by language\n",
    "dataset = dataset[\n",
    "                    (dataset.detected_lang == \"en\") |\n",
    "                    (~dataset.translated_question.isnull())\n",
    "                  ]\n",
    "\n",
    "eng = dataset[dataset.detected_lang == \"en\"].question.apply(preprocess).to_frame(name=\"text\")\n",
    "french = dataset[dataset.detected_lang == \"fr\"].translated_question.apply(preprocess).to_frame(name=\"text\")\n",
    "dataset[\"text\"] = eng.combine_first(french)\n",
    "\n",
    "# Preprocess questions\n",
    "\n",
    "print(f\"Dataset has {len(dataset)} english documents (translated and otherwise)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset[(dataset.language != \"en\") & (dataset.detected_lang == \"en\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset[(dataset.language == \"en\") & (dataset.detected_lang != \"en\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rules\n",
    "Note: The order of these rules matters in resolving conflicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistics\n",
    "dataset[\"statistics\"] = (\n",
    "                        dataset.text.str.contains(\"cases\",case=False)|\n",
    "                        dataset.text.str.contains(\"death\",case=False)|\n",
    "                        dataset.text.str.contains(\"died\",case=False)|\n",
    "                        dataset.text.str.contains(\"mortality rate\",case=False)|\n",
    "                        dataset.text.str.contains(\"death rate\",case=False)|\n",
    "                        dataset.text.str.contains(\"deadly\",case=False)|\n",
    "                        dataset.text.str.contains(\"statistic\",case=False)|\n",
    "                        (\n",
    "                            dataset.text.str.contains(\"how\",case=False)&\n",
    "                            dataset.text.str.contains(\"many\",case=False)&\n",
    "                            dataset.text.str.contains(\"people\",case=False)\n",
    "                        )\n",
    "                    ).apply(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"animals\"] = (\n",
    "                        dataset.text.str.contains(r\"\\b(?:animal|bird|cat|dog)s?\\b\",case=False)\n",
    "                    ).apply(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"caution\"] = (\n",
    "                        dataset.text.str.contains(\"prevent\",case=False)|\n",
    "                        dataset.text.str.contains(\"protect\",case=False)|\n",
    "                        dataset.text.str.contains(\"precaution\",case=False)|\n",
    "                        dataset.text.str.contains(\"safety\",case=False)|\n",
    "                        (\n",
    "                            dataset.text.str.contains(\"keep\",case=False)&\n",
    "                            dataset.text.str.contains(\"safe\",case=False)\n",
    "                        )\n",
    "                    ).apply(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"viruslife\"] = (\n",
    "                            (\n",
    "                                dataset.text.apply(fuzzy_match, pattern=\"(?:covid){e<=2}\")|\n",
    "                                dataset.text.str.contains(\"corona\",case=False)|\n",
    "                                dataset.text.str.contains(\"virus\",case=False)\n",
    "                            )&\n",
    "                            (\n",
    "                                dataset.text.str.contains(\"live|stay|survive\",case=False)\n",
    "                            )&\n",
    "                            (\n",
    "                                dataset.text.str.contains(\"on\",case=False)\n",
    "                            )\n",
    "                    ).apply(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"viruskill\"] = (\n",
    "                            (\n",
    "                                dataset.text.apply(fuzzy_match, pattern=\"(?:covid){e<=2}\")|\n",
    "                                dataset.text.str.contains(\"corona\",case=False)|\n",
    "                                dataset.text.str.contains(\"virus\",case=False)\n",
    "                            )&\n",
    "                            (\n",
    "                                dataset.text.str.contains(\"kills\",case=False)\n",
    "                            )\n",
    "                    ).apply(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"virusfight\"] = (\n",
    "                            (\n",
    "                                (\n",
    "                                    dataset.text.apply(fuzzy_match, pattern=\"(?:covid){e<=2}\")|\n",
    "                                    dataset.text.str.contains(\"corona\",case=False)|\n",
    "                                    dataset.text.str.contains(\"virus\",case=False)\n",
    "                                )&\n",
    "                                (\n",
    "                                    dataset.text.str.contains(\"fight\",case=False)\n",
    "                                )&\n",
    "                                (\n",
    "                                    dataset.text.str.contains(\"help\",case=False)\n",
    "                                )\n",
    "                            )|\n",
    "                            (\n",
    "                                dataset.text.str.contains(\"mask\",case=False)|\n",
    "                                dataset.text.str.contains(\"glove\",case=False)\n",
    "                            )\n",
    "                        ).apply(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"treatment\"] = (\n",
    "                        dataset.text.str.contains(\"treatment\",case=False)|\n",
    "                        dataset.text.str.contains(\"cure\",case=False)|\n",
    "                        dataset.text.str.contains(\"vaccine\",case=False)|\n",
    "                        dataset.text.str.contains(\"medic\",case=False)\n",
    "                    ).apply(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"incubation\"] = (\n",
    "                        dataset.text.str.contains(\"incubate\",case=False)|\n",
    "                        dataset.text.str.contains(\"incubation\",case=False)\n",
    "                    ).apply(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"nextsteps\"] = (\n",
    "                        dataset.text.str.contains(\"i have\",case=False) \n",
    "                    ).apply(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"guideme\"] = (\n",
    "                        dataset.text.str.contains(\"if\",case=False)\n",
    "                    ).apply(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset[\"hospital\"] = (\n",
    "# #                         dataset.text.str.contains(r\"\\bgo\\b\",case=False)&\n",
    "#                         (\n",
    "#                             dataset.text.str.contains(\"hospital\",case=False)|                            \n",
    "#                             dataset.text.str.contains(r\"\\bER\\b\",case=False)\n",
    "#                         )\n",
    "#                     ).apply(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dos and Donts\n",
    "dataset[\"lockdown\"] = (\n",
    "                        (\n",
    "                            (\n",
    "                                dataset.text.str.contains(\"go (?:on|to|for|out)\",case=False)|\n",
    "                                dataset.text.str.contains(\"walk\",case=False)\n",
    "                            )&\n",
    "                            (\n",
    "                                dataset.text.str.contains(\"allow\",case=False)|\n",
    "                                dataset.text.str.contains(\"can\",case=False)|\n",
    "                                dataset.text.str.contains(\"ok|okay\",case=False)|\n",
    "                                dataset.text.str.contains(\"should|shall\",case=False)\n",
    "                            )\n",
    "                        )|\n",
    "                        (\n",
    "                            dataset.text.str.contains(\"lockdown\",case=False)|\n",
    "                            dataset.text.str.contains(r\"\\bopen\\b\",case=False)|\n",
    "                            dataset.text.str.contains(r\"\\bclose\",case=False)\n",
    "                        )\n",
    "                    ).apply(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"infection\"] = (\n",
    "                        dataset.text.str.contains(\"infected\",case=False)|\n",
    "                        dataset.text.str.contains(\"infection\",case=False)\n",
    "                    ).apply(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"diff\"] = (\n",
    "                        dataset.text.str.contains(\"diff\",case=False)|\n",
    "                        dataset.text.apply(fuzzy_match, pattern=\"(?:distinguish){e<=3}\")\n",
    "                    ).apply(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"recover\"] = (\n",
    "                        dataset.text.str.contains(\"recover\",case=False)\n",
    "                    ).apply(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"test\"] = (\n",
    "                        dataset.text.str.contains(\"tested\",case=False)|\n",
    "                        dataset.text.str.contains(\"test\",case=False)\n",
    "                    ).apply(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"isolation\"] = (\n",
    "                            dataset.text.str.contains(r\"\\bisolat\",case=False)|\n",
    "                            dataset.text.str.contains(r\"\\bsocial dist\",case=False)\n",
    "                        ).apply(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"dangerisk\"] = (\n",
    "                            dataset.text.str.contains(\"dangerous\",case=False)|\n",
    "                            dataset.text.str.contains(\"risk\",case=False)\n",
    "                        ).apply(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"transmission\"] = (\n",
    "                            dataset.text.str.contains(\"transmi\",case=False)|\n",
    "                            dataset.text.str.contains(\"contract\",case=False)|\n",
    "                            dataset.text.str.contains(\"spread\",case=False)|\n",
    "                            dataset.text.apply(fuzzy_match, pattern=\"(?:airborne){e<=3}\")\n",
    "                        ).apply(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fuzzy Matching of 'Symptom' keyword (accounting for spelling errors)\n",
    "dataset[\"symptom\"] = (\n",
    "                        dataset.text.apply(fuzzy_match, pattern=\"(?:symptom){1<=e<=3}\")\n",
    "                    ).apply(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"about\"] = (\n",
    "                        (\n",
    "                            dataset.text.apply(fuzzy_match, pattern=\"(?:whats|what (?:is|s))\")\n",
    "                        ) & \n",
    "                        (\n",
    "                            dataset.text.apply(fuzzy_match, pattern=\"(?:covid){e<=2}\")|\n",
    "                            dataset.text.str.contains(\"corona\",case=False)\n",
    "                        )\n",
    "                    ).apply(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"future\"] = (\n",
    "                        (\n",
    "                            (\n",
    "                                dataset.text.str.contains(\"how\",case=False) &\n",
    "                                dataset.text.str.contains(\"long\",case=False)\n",
    "                            )|\n",
    "                            dataset.text.str.contains(\"when\",case=False)\n",
    "                        )&\n",
    "                            dataset.text.str.contains(\"will\",case=False)&\n",
    "                        (\n",
    "                            dataset.text.str.contains(\"last|end|over|normal|done\",case=False)\n",
    "                        )\n",
    "                    ).apply(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"past\"] = (\n",
    "                        (\n",
    "                            dataset.text.str.contains(\"how|when|where\",case=False) \n",
    "                        )&\n",
    "                            dataset.text.str.contains(\"did\",case=False)&\n",
    "                        (\n",
    "                            dataset.text.str.contains(\"start|begin|began\",case=False)\n",
    "                        )\n",
    "                    ).apply(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.rename(columns=new_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [col_name for col_name in dataset.columns.values.tolist() if \"-\" in col_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get total number of classes/categories this question qualifies for\n",
    "dataset[\"total\"] = dataset[features].sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describes how many questions quality for how many classes\n",
    "dataset.groupby(\"total\")[\"situation-stats\"].describe()[\"count\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Set default value\n",
    "dataset[\"cluster\"] = \"unclassified\"\n",
    "\n",
    "# For single features\n",
    "for col in features:\n",
    "    dataset[\"cluster\"][(dataset.total == 1) & (dataset[col] == True)] = col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resolving multiple classes\n",
    "for col in features[::-1]:\n",
    "    dataset[\"cluster\"][(dataset.total > 1) & (dataset[col] == True)] = col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the files\n",
    "path = f\"output/simple_{args.suffix}_{args.lang}.csv\"\n",
    "dataset.drop(features, axis=\"columns\").drop([\"text\",\"total\"], axis=\"columns\").to_csv(path)\n",
    "print(f\"Rules based output saved to {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Clustering stats\n",
    "dataset.groupby(\"cluster\")[\"question\"].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Length analysis for situations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSA and AHC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = dataset[dataset.cluster==\"unclassified\"][[\"question\", \"cluster\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "stopwords_list = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wn_lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "def bulk_tokenizer(texts):\n",
    "#      return [[wn_lemmatizer.lemmatize(token) for token in nltk.word_tokenize(text)] for text in texts]\n",
    "     return [nltk.word_tokenize(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.n_topics = 15\n",
    "args.dist_thresh = 0.7\n",
    "model = utils.text.representation.LSI(args, tokenizer=bulk_tokenizer)\n",
    "d[\"embedding\"] = model.generate_embedding(d.question, returnarray=False)\n",
    "\n",
    "# Cluster\n",
    "X = pd.DataFrame(d[\"embedding\"].values.tolist(), index= d.index).to_numpy()\n",
    "clustering = AgglomerativeClustering(n_clusters=None, compute_full_tree=True, distance_threshold=args.dist_thresh).fit(X)\n",
    "d[\"ahc_label\"] = clustering.labels_\n",
    "\n",
    "# Misc.\n",
    "args.n_clusters = len(d[\"ahc_label\"].unique())\n",
    "print(f\"Found {args.n_clusters} clusters\")\n",
    "d.groupby(\"ahc_label\")[\"question\"].count().sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.groupby(\"ahc_label\")[\"question\"].count().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for label in d.ahc_label.unique():\n",
    "#     print(f\"\\ncluster #{label}, count - {len(d[d.ahc_label==label])}\")\n",
    "#     print(d[d.ahc_label==label][:10].question.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.join(d[\"ahc_label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = f\"output/simpleLsa_{args.n_topics}n{args.dist_thresh}dt_{args.suffix}.csv\"\n",
    "dataset.to_csv(path)\n",
    "print(f\"AHC on top of rule based output saved to {path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterative Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
